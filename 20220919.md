# 1. Find Duplicate File in System https://leetcode.com/problems/find-duplicate-file-in-system/

Solution

- Hash tables: O(n) & S(n)

Code

- Java

```java

```

- Python

```python
class Solution:
    def findDuplicate(self, paths: List[str]) -> List[List[str]]:
        record = {}
        for path in paths:
            pathList = path.split(' ')
            d = pathList[0]
            for i in range(1, len(pathList)):
                filename, content = pathList[i].split('.txt')
                if content in record:
                    record[content].append(d+'/'+filename+'.txt')
                else:
                    record[content] = [d+'/'+filename+'.txt']
        res = [l for l in record.values() if len(l) > 1]
        return res
```

Follow up

- Imagine you are given a real file system, how will you search files? DFS or BFS?
    In a real file system, it's more common the case there are many files stored in one folder, instead of many levels of directories. So BFS could consume more memory, but it will be faster.

- If the file content is very large (GB level), how will you modify your solution?
    When the content is very large, we cannot hash the whole file. In this case, we will first compare the size of the file. And for the files with the same size, we only hash a small part of the files (e.g. 1kb using MD5/SHA256). And if hashes are still the same, we will compare their content byte by byte. 

- If you can only read the file by 1kb each time, how will you modify your solution?
    Same as above. We only hash 1kb of each file if their sizes are the same.

- What is the time complexity of your modified solution? What is the most time-consuming part and memory-consuming part of it? How to optimize?
    O(number of files ^ 2 * sizes of files). The most time-consuming part is comparing files byte by byte. The most memory-consuming part is store the hash tables for all files. The above procedure will optimize it, since we compare files by size first, only when sizes differ, we'll generate and compare hashes, and only when hashes are the same, we'll compare byte by byte.

- How to make sure the duplicated files you find are not false positive?
    We use several filters: file size, hash and byte-by-byte comparison